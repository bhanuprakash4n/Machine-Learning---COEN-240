{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_pca.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml58PJ9UDwRk"
      },
      "source": [
        "**Principal Component Analysis**\n",
        "\n",
        "You will implement dimensionality reduction with PCA.  \n",
        "\n",
        "1). Read iris_dataset.csv (4 features, hence 4 PCs)\n",
        "\n",
        "2). Find the principal components\n",
        "\n",
        "3). Recontruct the dataset (X_hat)\n",
        "\n",
        "4). Determine the accuracy of X_hat for 1 PC and 4 PCs using LDA classifier (provided below)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3DA-QxT0O6X"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import matplotlib.pyplot as plt\n",
        "from numpy import linalg as LA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "# Load data - 150 observations, 4 features, 3 classes, \n",
        "df = pd.read_csv(\"/iris_dataset.csv\", header=None)\n",
        "data = df.values"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J_I64r12CK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "498c7d1e-9ad7-4aa7-a33a-5f5e4790743e"
      },
      "source": [
        "## Setup\n",
        "\n",
        "# Shuffle data randomly\n",
        "shuffled_data = data;\n",
        "np.random.shuffle(shuffled_data)\n",
        "X = shuffled_data[:,0:4]  # 150x4\n",
        "y = shuffled_data[:,4]\n",
        "\n",
        "# Classification accuracy with the original dataset using LDA\n",
        "model_mean_scores = []\n",
        "model = LinearDiscriminantAnalysis().fit(X, y)\n",
        "scores = cross_val_score(model, X, y, cv=10)\n",
        "model_mean_scores.append(np.mean(scores))\n",
        "print('>> Average accuracy with the original dataset = {0:0.4f}'.format(model_mean_scores[0]))\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Average accuracy with the original dataset = 0.9800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3llQ6-RP00N"
      },
      "source": [
        "def evaluate_accuracy(X, X_hat, Num_PC, y):\n",
        "    \n",
        "  ######################################################################\n",
        "  # Evaluate reconstruction error and classificatin accuracy with LDA\n",
        "  ######################################################################\n",
        "  '''\n",
        "    Inputs:\n",
        "      X: original dataset, dimension=150x4\n",
        "      X_hat: reconstructed dataset. dimension=150x4\n",
        "      Num_PC: number of PC's used to recover X_hat\n",
        "      y: class label vector. dimension=150x1\n",
        "\n",
        "  '''\n",
        "  \n",
        "  from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "  from sklearn.model_selection import cross_val_score\n",
        "\n",
        "  no_dim = Num_PC  # number of dimensions\n",
        "  X_train = X_hat[:,0:Num_PC]        # dimensionally reduced dataset\n",
        "  y_train = y\n",
        "  recon_error = (np.linalg.norm((X-X_hat),'fro')) / (np.linalg.norm(X, 'fro'))\n",
        "\n",
        "  model_mean_scores = []\n",
        "  model = LinearDiscriminantAnalysis().fit(X_train, y_train)\n",
        "  scores = cross_val_score(model, X_train, y_train, cv=10)\n",
        "  model_mean_scores.append(np.mean(scores))\n",
        "\n",
        "  print('Reconstruction error = {0:0.6f} with {1:1d} PCs, average accuracy = {2:0.4f}'\n",
        "     .format(recon_error, Num_PC, model_mean_scores[0]))"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-L8WvJIAKeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77f9f130-669b-45de-a341-13d042b17293"
      },
      "source": [
        "### Your code goes here ...\n",
        "u = np.mean(X , axis = 0)\n",
        "XM = X - u\n",
        "\n",
        "cov = np.cov(XM , rowvar = False)\n",
        "\n",
        "eval , evect = np.linalg.eigh(cov)\n",
        "\n",
        "srtd_idx = np.argsort(eval)[::-1]\n",
        "srtd_eval = eval[srtd_idx]\n",
        "srtd_evect = evect[:,srtd_idx]\n",
        "\n",
        "Y = np.dot(XM, srtd_evect)\n",
        "\n",
        "## You must call the function evaluate_accurady \n",
        "X_hat = np.dot(Y[:,0:1], srtd_evect[:,0:1].transpose()) + u\n",
        "evaluate_accuracy(X, X_hat, 1, y)  # classification accuracy with 1 PC\n",
        "\n",
        "X_hat = np.dot(Y, srtd_evect.transpose()) + u\n",
        "evaluate_accuracy(X, X_hat, 4, y)  # classification accuracy with 4 PCs\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reconstruction error = 0.073378 with 1 PCs, average accuracy = 0.9267\n",
            "Reconstruction error = 0.000000 with 4 PCs, average accuracy = 0.9800\n"
          ]
        }
      ]
    }
  ]
}